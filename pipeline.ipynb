{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Key Ideas:\n",
    "# -> Load the train, test data frame.\n",
    "# -> Compute csr - train_csr.\n",
    "# -> Perform SVD on train_csr.\n",
    "# -> Perform NMF on train_csr.\n",
    "# -> Select transactions to be used for Training.\n",
    "# -> Collect the following features for the transactions:\n",
    "#     * SVD val for the pair\n",
    "#     * NMF val for the pair\n",
    "#     * In-degree for sender\n",
    "#     * Out-degree for sender\n",
    "#     * In-degree for reciever\n",
    "#     * Out-degree for reciever\n",
    "#     * PageRank sender \n",
    "#     * PageRank reciever\n",
    "#     * Part of same connected component?\n",
    "#     * Closeness Centrality for sender\n",
    "#     * Betweenness Centrality for sender\n",
    "#     * Closeness Centrality for reciever\n",
    "#     * Betweenness Centrality for reciever\n",
    "#     * Jaccards for the pair\n",
    "#     * Acad_ for the pair\n",
    "# -> Train Classifier\n",
    "# -> Collect same metrics from test transactions\n",
    "# -> Generate Predictions\n",
    "# -> Metrics: RoC, Accuracy, Confusion Matrix\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import csr_matrix, coo_matrix, linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(convert_to_bin_trans):\n",
    "    dftrain = pd.read_csv('data/txTripletsCounts.txt', header=None, index_col=None, sep=' ',\n",
    "                          names=['sender','receiver','transaction'])\n",
    "    dftest = pd.read_csv('data/testTriplets.txt', header=None, index_col=None, sep=' ',\n",
    "                         names=['sender','receiver','transaction'])\n",
    "    dim = max((dftrain['sender'].max(), dftrain['receiver'].max(), dftest['sender'].max(), dftest['receiver'].max()))\n",
    "    dim += 1\n",
    "    \n",
    "    if convert_to_bin_trans:\n",
    "        dftrain['transaction'] = np.array(dftrain['transaction'].tolist()).astype('bool').astype('int')\n",
    "    \n",
    "    train_csr = csr_matrix((dftrain['transaction'],(dftrain['sender'],dftrain['receiver'])), shape=(dim,dim), dtype=float)\n",
    "    return dftrain, dftest, train_csr\n",
    "\n",
    "\n",
    "def gen_nmf(A, n_components, save=False):\n",
    "    nmf_model = NMF(n_components=n_components)\n",
    "    W = nmf_model.fit_transform(A);\n",
    "    H = nmf_model.components_;\n",
    "    if save:\n",
    "        save_sparse_csr(\"data/nmf_W_\"+int(n_components), csr_matrix(W))\n",
    "        save_sparse_csr(\"data/nmf_H_\"+int(n_components), csr_matrix(H))\n",
    "    return W, H\n",
    "\n",
    "\n",
    "def get_predictions_svd(U, sigma, VT, df):\n",
    "    pred = [np.sum(U[row['sender'],:] * sigma * VT[:,row['receiver']]) \n",
    "        for index,row in df[['sender', 'receiver']].iterrows()]\n",
    "    return np.array(pred).astype(float)\n",
    "\n",
    "\n",
    "def get_predictions_nmf(W, H, df):\n",
    "    pred = [np.sum(W[row['sender'],:] * H[:,row['receiver']]) \n",
    "            for index,row in df[['sender', 'receiver']].iterrows()]\n",
    "    return np.array(pred).astype(float)\n",
    "\n",
    "\n",
    "def are_in_same_component(node1, node2, component_list):\n",
    "    for component in component_list:\n",
    "        if node1 in component or node2 in component:\n",
    "            return node1 in component and node2 in component\n",
    "\n",
    "        \n",
    "def compute_graph_metrics(g, un_dir_g):\n",
    "    in_d = nx.centrality.in_degree_centrality(g)\n",
    "    out_d = nx.centrality.out_degree_centrality(g)\n",
    "\n",
    "    pagerank = nx.pagerank(g)\n",
    "\n",
    "    closeness = nx.centrality.in_degree_centrality(g)\n",
    "    betweenness = nx.centrality.out_degree_centrality(g)\n",
    "\n",
    "    cc = list(nx.connected_components(un_dir_g))\n",
    "    scc = list(nx.strongly_connected_components(g))\n",
    "    wcc = list(nx.weakly_connected_components(g))\n",
    "    return in_d, out_d, pagerank, closeness, betweenness, cc, scc, wcc\n",
    "\n",
    "        \n",
    "def add_lin_alg_features(df, U, sigma, VT, W, H):\n",
    "    df['svd_vals'] = get_predictions_svd(U, sigma, VT, df)\n",
    "    df['nmf_vals'] = get_predictions_nmf(W, H, df)\n",
    "\n",
    "\n",
    "def add_centrality_features(df, in_d, out_d, closeness, betweenness, pagerank):\n",
    "    df['snd_in_degree'] = [in_d[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_in_degree'] = [in_d[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "    df['snd_out_degree'] = [out_d[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_out_degree'] = [out_d[i] for i in df['receiver'].tolist()]\n",
    "    \n",
    "    df['snd_closeness'] = [closeness[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_closeness'] = [closeness[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "    df['snd_betweenness'] = [betweenness[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_betweenness'] = [betweenness[i] for i in df['receiver'].tolist()]\n",
    "    \n",
    "    df['snd_pagerank'] = [pagerank[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_pagerank'] = [pagerank[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "\n",
    "def add_pair_features(df, pairs, un_dir_g, cc, scc, wcc):\n",
    "    df['adamic'] = [x[2] for x in list(nx.adamic_adar_index(un_dir_g, pairs))]\n",
    "    df['jaccard'] = [x[2] for x in list(nx.jaccard_coefficient(un_dir_g, pairs))]\n",
    "\n",
    "    df['connected'] = np.array([are_in_same_component(pr[0], pr[1], cc) for pr in pairs]).astype('int')\n",
    "\n",
    "    df['strng_connected'] = np.array([are_in_same_component(pr[0], pr[1], scc) for pr in pairs]).astype('int')\n",
    "\n",
    "    df['wk_connected'] = np.array([are_in_same_component(pr[0], pr[1], wcc) for pr in pairs]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "convert_to_bin_trans = True\n",
    "\n",
    "svd_k = 50\n",
    "nmf_n = 12\n",
    "\n",
    "selection_column = 'sender'\n",
    "selection_count = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the train, test data frame.\n",
    "dftrain, dftest, train_csr = load_data(convert_to_bin_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Done!\n",
      "NMF Done!\n",
      "G represent Done!\n"
     ]
    }
   ],
   "source": [
    "# Common computations for both train & test data\n",
    "\n",
    "# Perform SVD on train_csr.\n",
    "U, sigma, VT = svds(train_csr, k=svd_k, tol=1e-10, which = 'LM')\n",
    "print \"SVD Done!\"\n",
    "\n",
    "# Perform NMF on train_csr.\n",
    "W, H = gen_nmf(train_csr, nmf_n)\n",
    "print \"NMF Done!\"\n",
    "\n",
    "# Graph based Computations\n",
    "g = nx.from_scipy_sparse_matrix(train_csr, create_using=nx.DiGraph())\n",
    "un_dir_g = nx.from_scipy_sparse_matrix(train_csr)\n",
    "print \"G represent Done!\"\n",
    "\n",
    "in_d, out_d, pagerank, closeness, betweenness, cc, scc, wcc = compute_graph_metrics(g, un_dir_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_senders = np.random.choice(dftrain[selection_column].unique(), selection_count)\n",
    "train_txns = dftrain.loc[dftrain[selection_column].isin(random_senders)].copy(deep=True)\n",
    "pairs = zip(train_txns['sender'].tolist(), train_txns['receiver'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_lin_alg_features(train_txns, U, sigma, VT, W, H)\n",
    "add_centrality_features(train_txns, in_d, out_d, closeness, betweenness, pagerank)\n",
    "add_pair_features(train_txns, pairs, un_dir_g, cc, scc, wcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_lin_alg_features(dftest, U, sigma, VT, W, H)\n",
    "add_centrality_features(dftest, in_d, out_d, closeness, betweenness, pagerank)\n",
    "add_pair_features(dftest, pairs, un_dir_g, cc, scc, wcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
