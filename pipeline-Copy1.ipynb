{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Key Ideas:\n",
    "# -> Load the train, test data frame.\n",
    "# -> Compute csr - train_csr.\n",
    "# -> Perform SVD on train_csr.\n",
    "# -> Perform NMF on train_csr.\n",
    "# -> Select transactions to be used for Training.\n",
    "# -> Collect the following features for the transactions:\n",
    "#     * SVD val for the pair\n",
    "#     * NMF val for the pair\n",
    "#     * In-degree for sender\n",
    "#     * Out-degree for sender\n",
    "#     * In-degree for reciever\n",
    "#     * Out-degree for reciever\n",
    "#     * PageRank sender \n",
    "#     * PageRank reciever\n",
    "#     * Part of same connected component?\n",
    "#     * Closeness Centrality for sender\n",
    "#     * Betweenness Centrality for sender\n",
    "#     * Closeness Centrality for reciever\n",
    "#     * Betweenness Centrality for reciever\n",
    "#     * Jaccards for the pair\n",
    "#     * Acad_ for the pair\n",
    "# -> Train Classifier\n",
    "# -> Collect same metrics from test transactions\n",
    "# -> Generate Predictions\n",
    "# -> Metrics: RoC, Accuracy, Confusion Matrix\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import csr_matrix, coo_matrix, linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(convert_to_bin_trans):\n",
    "    dftrain = pd.read_csv('data/txTripletsCounts.txt', header=None, index_col=None, sep=' ',\n",
    "                          names=['sender','receiver','transaction'])\n",
    "    dftest = pd.read_csv('data/testTriplets.txt', header=None, index_col=None, sep=' ',\n",
    "                         names=['sender','receiver','transaction'])\n",
    "    dim = max((dftrain['sender'].max(), dftrain['receiver'].max(), dftest['sender'].max(), dftest['receiver'].max()))\n",
    "    dim += 1\n",
    "    \n",
    "    if convert_to_bin_trans:\n",
    "        dftrain['transaction'] = np.array(dftrain['transaction'].tolist()).astype('bool').astype('int')\n",
    "    \n",
    "    train_csr = csr_matrix((dftrain['transaction'],(dftrain['sender'],dftrain['receiver'])), shape=(dim,dim), dtype=float)\n",
    "    return dftrain, dftest, train_csr\n",
    "\n",
    "\n",
    "def gen_nmf(A, n_components, save=False):\n",
    "    nmf_model = NMF(n_components=n_components)\n",
    "    W = nmf_model.fit_transform(A);\n",
    "    H = nmf_model.components_;\n",
    "    if save:\n",
    "        save_sparse_csr(\"data/nmf_W_\"+int(n_components), csr_matrix(W))\n",
    "        save_sparse_csr(\"data/nmf_H_\"+int(n_components), csr_matrix(H))\n",
    "    return W, H\n",
    "\n",
    "\n",
    "def get_predictions_svd(U, sigma, VT, df):\n",
    "    pred = [np.sum(U[row['sender'],:] * sigma * VT[:,row['receiver']]) \n",
    "        for index,row in df[['sender', 'receiver']].iterrows()]\n",
    "    return np.array(pred).astype(float)\n",
    "\n",
    "\n",
    "def get_predictions_nmf(W, H, df):\n",
    "    pred = [np.sum(W[row['sender'],:] * H[:,row['receiver']]) \n",
    "            for index,row in df[['sender', 'receiver']].iterrows()]\n",
    "    return np.array(pred).astype(float)\n",
    "\n",
    "\n",
    "def are_in_same_component(node1, node2, component_list):\n",
    "    for component in component_list:\n",
    "        if node1 in component or node2 in component:\n",
    "            return node1 in component and node2 in component\n",
    "\n",
    "        \n",
    "def compute_graph_metrics(g, un_dir_g):\n",
    "    print \"Graph Centrality measures!\"\n",
    "    in_d = nx.centrality.in_degree_centrality(g)\n",
    "    out_d = nx.centrality.out_degree_centrality(g)\n",
    "\n",
    "    print \"Graph PageRank!\"\n",
    "    pagerank = nx.pagerank(g)\n",
    "\n",
    "    print \"Graph Closeness & Betweenness!\"\n",
    "    closeness = nx.centrality.in_degree_centrality(g)\n",
    "    betweenness = nx.centrality.out_degree_centrality(g)\n",
    "\n",
    "    print \"Graph Connected Comps!\"\n",
    "    cc = list(nx.connected_components(un_dir_g))\n",
    "    print \"Graph Strongly Connected Comps!\"\n",
    "    scc = list(nx.strongly_connected_components(g))\n",
    "    print \"Graph Weakly Connected Comps!\"\n",
    "    wcc = list(nx.weakly_connected_components(g))\n",
    "    return in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc\n",
    "\n",
    "        \n",
    "def add_lin_alg_features(df, U, sigma, VT, W, H):\n",
    "    df['svd_vals'] = get_predictions_svd(U, sigma, VT, df)\n",
    "    df['nmf_vals'] = get_predictions_nmf(W, H, df)\n",
    "\n",
    "\n",
    "def add_centrality_features(df, in_d, out_d, closeness, betweenness, pagerank):\n",
    "    df['snd_in_degree'] = [in_d[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_in_degree'] = [in_d[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "    df['snd_out_degree'] = [out_d[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_out_degree'] = [out_d[i] for i in df['receiver'].tolist()]\n",
    "    \n",
    "    df['snd_closeness'] = [closeness[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_closeness'] = [closeness[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "    df['snd_betweenness'] = [betweenness[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_betweenness'] = [betweenness[i] for i in df['receiver'].tolist()]\n",
    "    \n",
    "    df['snd_pagerank'] = [pagerank[i] for i in df['sender'].tolist()]\n",
    "    df['rcv_pagerank'] = [pagerank[i] for i in df['receiver'].tolist()]\n",
    "\n",
    "\n",
    "def add_pair_features(df, pairs, un_dir_g, cc, scc, wcc):\n",
    "    df['adamic'] = [x[2] for x in list(nx.adamic_adar_index(un_dir_g, pairs))]\n",
    "    df['jaccard'] = [x[2] for x in list(nx.jaccard_coefficient(un_dir_g, pairs))]\n",
    "\n",
    "    df['connected'] = np.array([are_in_same_component(pr[0], pr[1], cc) for pr in pairs]).astype('int')\n",
    "\n",
    "    df['strng_connected'] = np.array([are_in_same_component(pr[0], pr[1], scc) for pr in pairs]).astype('int')\n",
    "\n",
    "    df['wk_connected'] = np.array([are_in_same_component(pr[0], pr[1], wcc) for pr in pairs]).astype('int')\n",
    "\n",
    "\n",
    "def get_random_traindf(dftrain, sender_count, receiver_count):\n",
    "    random_senders = np.random.choice(dftrain['sender'].unique(), sender_count)\n",
    "    random_receivers = np.random.choice(dftrain['receiver'].unique(), receiver_count)\n",
    "\n",
    "    sender_ttxns = dftrain.loc[dftrain['sender'].isin(random_senders)]\n",
    "    receiver_ttxns = dftrain.loc[dftrain['receiver'].isin(random_receivers)]\n",
    "\n",
    "    ttxns = pd.concat([sender_ttxns, receiver_ttxns]).copy(deep=True)\n",
    "    temp_pairs = zip(ttxns['sender'].tolist(), ttxns['receiver'].tolist())\n",
    "    \n",
    "    negative_txns_pairs = []\n",
    "    for send, rec in itertools.product(range(len(random_senders)), range(len(random_receivers))):\n",
    "        if (random_senders[send], random_receivers[rec]) not in temp_pairs:\n",
    "            negative_txns_pairs.append([random_senders[send], random_receivers[rec], int(train_csr[random_senders[send], random_receivers[rec]])])\n",
    "\n",
    "    ttxns = ttxns.append(pd.DataFrame(negative_txns_pairs, columns=['sender','receiver','transaction']))\n",
    "    pairs = zip(ttxns['sender'].tolist(), ttxns['receiver'].tolist())\n",
    "   \n",
    "    return ttxns, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_graph_cals(train_csr):\n",
    "    # Graph based Computations\n",
    "    g = nx.from_scipy_sparse_matrix(train_csr, create_using=nx.DiGraph())\n",
    "    un_dir_g = nx.from_scipy_sparse_matrix(train_csr)\n",
    "    print \"G represent Done!\"\n",
    "    in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc = compute_graph_metrics(g, un_dir_g)\n",
    "    print \"Graph calcs Done!\"\n",
    "    return g, un_dir_g, in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_feature_dfs(dftrain, dftest, train_csr, \n",
    "                    convert_to_bin_trans, svd_k, nmf_n, unq_senders, unq_receivers, \n",
    "                    g, un_dir_g, in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc):\n",
    "    # Training Txns\n",
    "    print \"Training Txns selection!\"\n",
    "    train_txns, pairs = get_random_traindf(dftrain, unq_senders, unq_receivers)\n",
    "    # Test Txn pairs\n",
    "    test_pairs = zip(dftest['sender'].tolist(), dftest['receiver'].tolist())\n",
    "    \n",
    "    # Common computations for both train & test data\n",
    "    # Perform SVD on train_csr.\n",
    "    print \"SVD!\"\n",
    "    U, sigma, VT = svds(train_csr, k=svd_k, tol=1e-10, which = 'LM')\n",
    "    # Perform NMF on train_csr.\n",
    "    print \"NMF!\"\n",
    "    W, H = gen_nmf(train_csr, nmf_n)\n",
    "    \n",
    "    # Add Train Features\n",
    "    print \"Adding Train Features\"\n",
    "    add_lin_alg_features(train_txns, U, sigma, VT, W, H)\n",
    "    add_centrality_features(train_txns, in_d, out_d, closeness, betweenness, pagerank)\n",
    "    add_pair_features(train_txns, pairs, un_dir_g, cc, scc, wcc)\n",
    "    \n",
    "    # Add Test Features\n",
    "    print \"Adding Test Features\"\n",
    "    add_lin_alg_features(dftest, U, sigma, VT, W, H)\n",
    "    add_centrality_features(dftest, in_d, out_d, closeness, betweenness, pagerank)\n",
    "    add_pair_features(dftest, test_pairs, un_dir_g, cc, scc, wcc)\n",
    "    \n",
    "    # Save Data frames\n",
    "    print \"Saving DFs\"\n",
    "    filename_postfix = '_bin{}_svd{}_nmf{}_snd{}_rcv{}.csv'.format(str(convert_to_bin_trans), str(svd_k), str(nmf_n), str(unq_senders), str(unq_receivers))\n",
    "    train_txns.to_csv('features/train' + filename_postfix)\n",
    "    dftest.to_csv('features/test' + filename_postfix)\n",
    "    \n",
    "    return train_txns, dftest\n",
    "\n",
    "\n",
    "def load_features_file(convert_to_bin_trans, svd_k, nmf_n, unq_senders, unq_receivers):\n",
    "    filename_postfix = '_bin{}_svd{}_nmf{}_snd{}_rcv{}.csv'.format(str(convert_to_bin_trans), str(svd_k), str(nmf_n), str(unq_senders), str(unq_receivers))\n",
    "    ftrain = pd.read_csv('features/train' + filename_postfix, index_col=0)\n",
    "    ftest = pd.read_csv('features/test' + filename_postfix, index_col=0)\n",
    "    return ftrain, ftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "convert_to_bin_trans = True\n",
    "\n",
    "svd_k = 50\n",
    "nmf_n = 12\n",
    "\n",
    "unq_senders = 250\n",
    "unq_receivers = 100\n",
    "\n",
    "# Load the train, test data frame.\n",
    "dftrain, dftest, train_csr = load_data(convert_to_bin_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G represent Done!\n",
      "Graph calcs Done!\n",
      "Training Txns selection Done!\n",
      "SVD Done!\n",
      "NMF Done!\n"
     ]
    }
   ],
   "source": [
    "g, un_dir_g, in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc = do_graph_cals(train_csr)\n",
    "gen_traindf, gen_testdf = gen_feature_dfs(dftrain, dftest, train_csr, \n",
    "                                          convert_to_bin_trans, svd_k, nmf_n, unq_senders, unq_receivers,\n",
    "                                          g, un_dir_g, in_d, out_d, pagerank, closeness, betweenness, cc, wcc, scc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fl_traindf, fl_testdf = load_features_file(convert_to_bin_trans, svd_k, nmf_n, unq_senders, unq_receivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26430, 20) (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "print gen_traindf.shape, gen_testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26430, 20) (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "print fl_traindf.shape, fl_testdf.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
